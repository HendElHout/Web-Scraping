{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "503b5c09-ab14-4cd2-aaba-7f26870c405e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been extracted and saved to Extract_Text_Data.csv\n",
      "Table data has been extracted and saved to Extract_Table_Data.csv\n",
      "Data scraping completed successfully.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "url = 'https://www.baraasallout.com/test.html'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#1\n",
    "headings = [h.get_text().strip() for h in soup.find_all(['h1', 'h2'])]\n",
    "paragraphs = [tag.get_text().strip() for tag in soup.find_all('p')]\n",
    "list_items = [tag.get_text().strip() for tag in soup.find_all('li')]\n",
    "with open('Extract_Text_Data.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Section', 'Content'])\n",
    "    writer.writerow(['Headings', ''])\n",
    "    for heading in headings:\n",
    "        writer.writerow(['', heading])\n",
    "    writer.writerow(['Paragraphs', ''])\n",
    "    for paragraph in paragraphs:\n",
    "        writer.writerow(['', paragraph])\n",
    "    writer.writerow(['List Items', ''])\n",
    "    for list_item in list_items:\n",
    "        writer.writerow(['', list_item])\n",
    "print(\"Data has been extracted and saved to Extract_Text_Data.csv\")\n",
    "\n",
    "#2\n",
    "table_rows = soup.find_all('tr')\n",
    "table_data = []\n",
    "\n",
    "for row in table_rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [col.get_text().strip() for col in cols]\n",
    "    if len(cols) == 3:  \n",
    "        table_data.append(cols)\n",
    "with open('Extract_Table_Data.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Product Name', 'Price', 'Stock Status'])\n",
    "    for data in table_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(\"Table data has been extracted and saved to Extract_Table_Data.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#3\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.baraasallout.com/test.html'\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    product_info = []\n",
    "    for card in soup.find_all('div', class_='product-card'):\n",
    "        title = card.find('h3').get_text(strip=True) if card.find('h3') else 'No Title'\n",
    "        price = card.find('span', class_='price').get_text(strip=True) if card.find('span', class_='price') else 'No Price'\n",
    "        stock = card.find('span', class_='stock-status').get_text(strip=True) if card.find('span', class_='stock-status') else 'No Stock Status'\n",
    "        button_text = card.find('button').get_text(strip=True) if card.find('button') else 'No Button'\n",
    "        \n",
    "        product_info.append({\n",
    "            'title': title,\n",
    "            'price': price,\n",
    "            'stock': stock,\n",
    "            'button_text': button_text\n",
    "        })\n",
    "\n",
    "\n",
    "    with open('Product_Information.json', 'w') as json_file:\n",
    "        json.dump(product_info, json_file, indent=4)\n",
    "\n",
    "    form_details = []\n",
    "    for form in soup.find_all('form'):\n",
    "        for input_tag in form.find_all('input'):\n",
    "            input_data = {\n",
    "                'field_name': input_tag.get('name'),\n",
    "                'input_type': input_tag.get('type'),\n",
    "                'default_value': input_tag.get('value', '')\n",
    "            }\n",
    "            form_details.append(input_data)\n",
    "\n",
    "\n",
    "    with open('Form_Details.json', 'w') as json_file:\n",
    "        json.dump(form_details, json_file, indent=4)\n",
    "\n",
    "    print(\"Data scraping completed successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage: {response.status_code}\")\n",
    "\n",
    "    forms = soup.find_all('form')\n",
    "    form_details = []\n",
    "\n",
    "    for form in forms:\n",
    "        inputs = form.find_all('input')\n",
    "        for input_tag in inputs:\n",
    "            input_data = {\n",
    "                'field_name': input_tag.get('name'),\n",
    "                'input_type': input_tag.get('type'),\n",
    "                'default_value': input_tag.get('value', '')\n",
    "            }\n",
    "            form_details.append(input_data)\n",
    "\n",
    "    with open('Form_Details.json', 'w') as json_file:\n",
    "        json.dump(form_details, json_file, indent=4)\n",
    "\n",
    "    print(\"Data scraping completed successfully.\")\n",
    "    \n",
    "forms = soup.find_all('form')\n",
    "form_details = []\n",
    "\n",
    "for form in forms:\n",
    "    inputs = form.find_all('input')\n",
    "    for input_tag in inputs:\n",
    "        input_data = {\n",
    "            'field_name': input_tag.get('name'),\n",
    "            'input_type': input_tag.get('type'),\n",
    "            'default_value': input_tag.get('value', '')\n",
    "        }\n",
    "        form_details.append(input_data)\n",
    "with open('Form_Details.json', 'w') as json_file:\n",
    "    json.dump(form_details, json_file, indent=4)\n",
    "links = soup.find_all('a', href=True)\n",
    "iframes = soup.find_all('iframe', src=True)\n",
    "\n",
    "link_data = [{'text': link.get_text(), 'href': link['href']} for link in links]\n",
    "video_data = [{'src': iframe['src']} for iframe in iframes]\n",
    "all_data = {\n",
    "    'links': link_data,\n",
    "    'videos': video_data\n",
    "}\n",
    "\n",
    "with open('Links_and_Multimedia.json', 'w') as json_file:\n",
    "    json.dump(all_data, json_file, indent=4)\n",
    "\n",
    "featured_products = soup.find_all('span', class_='name')\n",
    "products_data = []\n",
    "\n",
    "for product in featured_products:\n",
    "    product_name = product.get_text()\n",
    "    price = product.find_next('span', class_='price').get_text()  # Hidden price\n",
    "    colors = product.find_next('span', class_='colors').get_text()\n",
    "    product_id = product.find_parent('div').get('data-id')\n",
    "    \n",
    "    products_data.append({\n",
    "        'id': product_id,\n",
    "        'name': product_name,\n",
    "        'price': price,\n",
    "        'colors': colors\n",
    "    })\n",
    "print(products_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe9033-5446-4e99-b851-e1eb481df3ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
